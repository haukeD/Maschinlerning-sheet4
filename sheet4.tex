%inizative bildverarbeitung
%llncs    article
\documentclass[a4paper,parskip=full-]{article}

\usepackage{amsmath}
\usepackage{amssymb}

%http://de.wikibooks.org/wiki/LaTeX-W%C3%B6rterbuch:_Anf%C3%BChrungszeichen
\usepackage[autostyle=true,german=quotes]{csquotes}

\usepackage{subcaption}
\usepackage{float}
\restylefloat{figure}
\usepackage{graphicx}  % images
%\usepackage{qtree} % for trees
%auch für Bäume
%http://tex.stackexchange.com/questions/183866/tree-with-six-or-more-children
\usepackage{tikz, tikz-qtree}
\usepackage{pgfplots}
\pgfplotsset{grid style={dashed,gray}}
\usetikzlibrary{automata,topaths,plotmarks}
\usepackage{lmodern}  %THE tex font :)
\usepackage{url}  %urls in references
%\usepackage{prettyref}
%\usepackage{pstricks} %graphicv2
\usepackage{cite}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{xspace}

%http://www.kkittel.de/wiki/doku.php?id=tabellen:farbige_zellen
\usepackage{colortbl}

\usepackage{algorithmic} 
\usepackage{wasysym}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc} % Zeichenkodierung   utf8   latin1
%\include{biblio} % references
\usepackage{listings}                           % for source code inclusion
\usepackage{multirow} 
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{color}

\usepackage{fancyvrb }

%absatz
\usepackage{setspace} 

%breaking math formulars automaticly
%http://tex.stackexchange.com/questions/3782/how-can-i-split-an-equation-over-two-lines
\usepackage{breqn}

%für kurze Enumarates wie i,I a, A etc.
\usepackage[shortlabels]{enumitem}

%Durchstreichungen
%\cancel
%http://de.wikibooks.org/wiki/LaTeX-Kompendium:_F%C3%BCr_Mathematiker#Durchstreichungen
\usepackage{cancel}

%Für Römische Zahlen
\usepackage{enumitem}
%\usepackage{romannum}%stdclsdv

%Durchstreicen möglich
\usepackage[normalem]{ulem}

%Bessere Brüche
\usepackage{nicefrac}

%bookmarks
%\usepackage[pdftex,bookmarks=true]{hyperref}
%[pdftex,bookmarks=true,bookmarksopen,bookmarksdepth=2]
\usepackage{hyperref}
%\usepackage{scrhack}

%fußnoten
\usepackage{footnote}
\usepackage{caption} 

\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=25mm,bmargin=25mm,lmargin=15mm,rmargin=20mm}

%randnotiz
\newcommand\mpar[1]{\marginpar {\flushleft\sffamily\small #1}}
\setlength{\marginparwidth}{3cm}

%svg Grafiken
%http://tex.stackexchange.com/questions/122871/include-svg-images-with-the-svg-package
%\usepackage{svg}

\usepackage{pgf}

%http://tex.stackexchange.com/questions/48653/making-subsections-be-numbered-with-a-b-c
\usepackage{chngcntr}
\counterwithin{subsection}{section}

%Sektions nicht Nummerrrieren (<=> section*{...})
% \makeatletter
% \renewcommand\@seccntformat[1]{}
% \makeatother
\setcounter{secnumdepth}{0}

\title{Machine Learning \\
Exercise sheet 4}

\author{Gruppe 9: \\Hauke Wree and Fridtjof Schulte Steinberg}

\newcommand{\R}[0]{{\mathbb{R}}}

\newcommand{\N}[0]{{\mathbb{N}}}
\newcommand{\C}[0]{{\mathbb{C}}}
\newcommand{\K}[0]{{\mathbb{K}}}
\newcommand{\lF}[0]{{\mathcal{l}}}
\newcommand{\I}[0]{{\mathcal{I}}}
\newcommand{\nhnh}[0]{{\frac{n}{2} \times \frac{n}{2}}} %nice
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%\newcommand{\rm}[1]{\romannumeral #1}
\newcommand{\RM}[1]{\MakeUppercase{\romannumeral #1{.}}} 

\renewcommand \thesubsection{\alph{subsection}}

\begin{document}

\maketitle

\section{Exercise 1 (Decision trees for Boolean functions):}

\subsection{a)}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}

\begin{tabular}{|c|c|c|}
\hline
A & B & $A \land \neg B$ \\
\hline
0 & 0 & 0 \\
\hline
0 & 1 & 0 \\
\hline
1 & 0 & 1 \\
\hline
1 & 1 & 0 \\
\hline
\end{tabular}

\end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}

\begin{tikzpicture}[every tree node/.style={draw,circle},
   level distance=1.25cm,sibling distance=1cm,
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}]
\Tree
[.A
    \edge node[auto=right,pos=.6] {0};
    [.no ]
    \edge node[auto=right,pos=.6] {1};
    [.B 
        \edge node[auto=right,pos=.8] {0};
        [.yes ]
        \edge node[auto=right,pos=.8] {1};
        [.no ]
    ]
]
\end{tikzpicture}
\end{subfigure}

\end{figure}

%b)
\subsection{b)}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}

\begin{tabular}{|c|c|c|c|c|}
\hline
A & B & C & $B \land C$  & $A \lor [B \land B$] \\
\hline
0 & 0 & 0 & 0 & 0\\
\hline
0 & 0 & 1 & 0 & 0\\
\hline
0 & 1 & 0 & 0 & 0\\
\hline
0 & 1 & 1 & 1 & 1\\
\hline
1 & 0 & 0 & 0 & 1\\
\hline
1 & 0 & 1 & 0 & 1\\
\hline
1 & 1 & 0 & 0 & 1\\
\hline
1 & 1 & 1 & 1 & 1\\
\hline
\end{tabular}

\end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}

\begin{tikzpicture}[every tree node/.style={draw,circle},
   level distance=1.25cm,sibling distance=1cm,
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}]
\Tree
[.A
    \edge node[auto=right] {$0$};
    [.B 
       \edge node[midway,left] {$0$};
       [.no ]
       \edge node[midway,right] {$1$};
       [.C 
         \edge node[midway,left] {$0$};
         [.no ]
         \edge node[midway,right] {$1$};
         [.yes ]
       ]
        ]
    \edge node[auto=left] {$1$};
    [.yes
        ]
]
\end{tikzpicture}

\end{subfigure}

\end{figure}

%c)
\subsection{c)}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}

\begin{tabular}{|c|c|c|}
\hline
A & B & $A XOR B$ \\
\hline
0 & 0 & 0 \\
\hline
0 & 1 & 1 \\
\hline
1 & 0 & 1 \\
\hline
1 & 1 & 0 \\
\hline
\end{tabular}

\end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}

\begin{tikzpicture}[every tree node/.style={draw,circle},
   level distance=1.25cm,sibling distance=1cm,
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}]
\Tree
[.A
    \edge node[auto=right] {$0$};
    [.B 
       \edge node[midway,left] {$0$};
       [.no ]
       \edge node[midway,right] {$1$};
       [.yes ]
        ]
    \edge node[auto=left] {$1$};
    [.B 
        \edge node[midway,left] {$0$};
        [.yes ]
        \edge node[midway,right] {$1$};
        [.no ]
        ]
]
\end{tikzpicture}

\end{subfigure}

\end{figure}

%d)
\subsection{d)}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
A & B & C & D & $A \land B$ & $C \land D$ & $[A \land B] \lor [C \land D]$ \\
\hline
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
0 & 0 & 0 & 1 & 0 & 0 & 0  \\
\hline
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
\hline
0 & 0 & 1 & 1 & 0 & 1 & 1 \\
\hline
0 & 1 & 0 & 0 & 0 & 0 & 0 \\
\hline
0 & 1 & 0 & 1 & 0 & 0 & 0 \\
\hline
0 & 1 & 1 & 0 & 0 & 0 & 0 \\
\hline
0 & 1 & 1 & 1 & 0 & 1 & 1 \\
\hline
1 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
1 & 0 & 0 & 1 & 0 & 0 & 0 \\
\hline
1 & 0 & 1 & 0 & 0 & 0 & 0 \\
\hline
1 & 0 & 1 & 1 & 0 & 1 & 1 \\
\hline
1 & 1 & 0 & 0 & 1 & 0 & 1 \\
\hline
1 & 1 & 0 & 1 & 1 & 0 & 1 \\
\hline
1 & 1 & 1 & 0 & 1 & 0 & 1 \\
\hline
1 & 1 & 1 & 1 & 1 & 1 & 1 \\
\hline

\end{tabular}

\end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}

\begin{tikzpicture}[every tree node/.style={draw,circle},
   level distance=1.25cm,sibling distance=1cm,
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}]
\Tree
[.A
    \edge node[auto=right] {$0$};
    [.C 
       \edge node[midway,left] {$0$};
       [.no ]
       \edge node[midway,right] {$1$};
       [.D 
         \edge node[midway,left] {$0$};
         [.no ]
         \edge node[midway,right] {$1$};
         [.yes ]
       ]
        ]
    \edge node[auto=left] {$1$};
    [.B 
          \edge node[midway,left] {$0$};
          [.no ]
          \edge node[midway,right] {$1$};
          [.yes ]
    ]
]
\end{tikzpicture}

\end{subfigure}

\end{figure}

\section{Exercise 2 (Decision tree learning)}

\subsection{a)}

\subsubsection{i.}

\begin{tikzpicture}[every tree node/.style={draw,circle},
   level distance=2.25cm,sibling distance=2cm,
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}]
\Tree
[.{Deadline ?}
    \edge node[auto=right] {Urgent};
    [.{Party ?} 
       \edge node[midway,left] {no};
       [.Study ]
       \edge node[midway,right] {yes};
       [.Party ]
    ]
    \edge node[auto=left] {Near};
    [.{Party ?} 
        \edge node[midway,left] {no};
        [.Study ]
        \edge node[midway,right] {yes};
        [.Party ]
    ]
    \edge node[auto=left] {None};
    [.{Party ?} 
        \edge node[midway,left] {no};
        [.Study ]
        \edge node[midway,right] {yes};
        [.Party ]
    ]
]
\end{tikzpicture}


\subsubsection{ii.}

\begin{tikzpicture}[every tree node/.style={draw,circle},
   level distance=2.25cm,sibling distance=2cm,
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}]
\Tree
[.{Party ?}
    \edge node[auto=right] {No};
    [.{Deadline ?} 
       \edge node[midway,left] {Urgent};
       [.Study ]
       \edge node[midway,right] {Near};
       [.Study ]
       \edge node[midway,right] {None};
        [.Study ]
    ]
    \edge node[auto=left] {Yes};
    [.{Deadline ?} 
        \edge node[midway,left] {Urgent};
        [.Party ]
        \edge node[midway,right] {Near};
        [.Party ]
        \edge node[midway,right] {None};
        [.Party ]
    ]
]
\end{tikzpicture}

\subsubsection{Compare both decision trees.}

Der Wert der Deadline ist für das Ergebnis irrelevant nur der Wert von \textit{Is there a party?} ist für das Ergebnis relevant, so dass der 
Entscheidungsbaum auch so sein kann.


\begin{figure}[H]
\caption{Vereinfachter Entscheidungsbaum}
\label{fig:3a2}
\begin{tikzpicture}[every tree node/.style={draw,circle},
   level distance=2.25cm,sibling distance=2cm,
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}]
\Tree
[.{Party ?}
    \edge node[auto=right] {No};
    [.Study ]
    \edge node[auto=left] {Yes};
    [.Party ]
]
\end{tikzpicture}
\end{figure}

\subsection{b)}

Wir haben zwei Klassen:

\begin{tabbing} 
S : Klassen \= \enquote{Study} \\
\> \enquote{Party}
\end{tabbing}

Die Entropie von $S$ ist:

\begin{equation*}
\begin{split}
H(S) = -ld (P(\text{\enquote{Party}})) \cdot P("Party") - ld (P((\text{\enquote{Study}})) \cdot P((\text{\enquote{Study}}) \\
\approx -\frac{5}{11} \cdot -1,137503523749935
   -\frac{6}{11} \cdot -0,874469117916141 
= 0,994030211476957
\end{split}
\end{equation*}

Wir nehmen das Attribut \enquote{Deadline} und wir teilen die Trainingsdaten in 3 Mengen $(S_1,S_2,S_3)$ auf.
Mit: \\
$S_1$ : Urgent \\
$S_2$ : Near \\
$S_3$ : None \\
dann sind die Entropien:  

$$H(S_1) = -\frac{1}{3} \cdot ld \left(\frac{1}{3} \right) - \frac{2}{3} \cdot ld \left(\frac{1}{3} \right) \approx 0,91829583405449 
$$
 
$$
H(S_2) = -\frac{2}{5} \cdot ld \left(\frac{2}{5} \right) - \frac{3}{5} \cdot ld \left(\frac{2}{5} \right) \approx 0,970950594454669 
$$
 
$$
H(S_3) = -\frac{1}{3} \cdot ld \left(\frac{1}{3} \right) - \frac{2}{3} \cdot ld \left(\frac{1}{3} \right) \approx 0,91829583405449 
$$

Wir teilen die Trainingsdaten für das Attribut $B$ \enquote{Is there a party?}, in Zwei Mengen $$(S_1, S_2)$$ auf, dann sind die Entropien:

$$
H(S_1) = -\frac{5}{5} \cdot ld(\frac{5}{5}) - \frac{0}{5} \cdot ld(\frac{0}{5}) = 0
$$

$$
H(S_2) = - \frac{0}{6} \cdot ld(\frac{0}{6}) -\frac{6}{6} \cdot ld(\frac{6}{6}) = 0
$$

\begin{flalign*}
%\begin{equation*}
%\begin{split}
%\begin{aligned}
&Gain(S,A) = H(S) - \sum_{v \in values(A)} \frac{|S_v|}{|S|} H(S_v) & \\
&= 0,994030211476957 - \frac{1}{11}\left(3 \cdot 0,91829583405449 +  5 \cdot 0,970950594454669 +  3 \cdot 0,91829583405449 \right) = & \\
&0,994030211476957 - 0,94222981605457 = 0,05180039542239 & \\
%\end{aligned}
%\end{split}
%\end{equation*}
\end{flalign*}


$$
Gain(S,B) = H(S) - \sum_{v \in values(B)} \frac{|S_v|}{|S|} H(S_v) = H(S) - 0 = 0,994030211476957
$$

Da $Gain(S,B) > Gain(S,A)$ maximiert $Gain(S,B)$ den \textit{information gain}, somit sollte das Attribut $B$ \enquote{Is there a party?}
zum Aufteilen gewählt werden.
\subsection{c)}

Das Programm \textit{dTree} gibt mit folgender Eingabe:

\begin{figure}[H]
\caption{Trainings- und Testdaten für dTree}
\label{fig:dTree2c}
\VerbatimInput[baselinestretch=1,fontsize=\footnotesize]{2c.txt}
\end{figure}

für Gini, Information Gain, Gain Ration und Random folgendes aus:

\begin{figure}[H]
   \centering
	\begin{minipage}[b]{0.45\textwidth}
  		\centering
		\includegraphics[width=\columnwidth]{3c-Gini}
		\label{fig:3c-gini}
		\caption{Gini}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\textwidth}
  		\centering
		\includegraphics[width=\columnwidth]{3c-Information-Gain} 
		\caption{Information-Gain}
	\end{minipage} \\
	\begin{minipage}[b]{0.45\textwidth}
  		\centering
    	\includegraphics[width=\columnwidth]{3c-Gain-Ration}
    	\caption{Information-Gain}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
  		\centering
    	\includegraphics[width=\columnwidth]{3c-Random}
    	\caption{Information-Gain}
    \end{minipage}
    \caption{Decision Tree Options für Gini (links oben), Information Gain (rechts oben), 
    Gain Ration (links unten) und Random (rechts unten)}
    \label{fig:3c}
\end{figure}

Wie in \autoref{fig:3c} zu sehen ist, ist nur beim Random-Algorithmus ein unterschied zu sehen.

%\begin{tikzpicture}[every tree node/.style={draw,circle},
%   level distance=2.25cm,sibling distance=2cm,
%   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)},scale=.55]
%\Tree
%[.{Deadline ?}
%    \edge node[auto=right] {Urgent};
%    [.{Party ?} 
%       \edge node[midway,left] {no};
%       [.{Lazy ?} 
%         \edge node[midway,left] {no};
%         [.Study ]
%         \edge node[midway,left] {yes};
%         [.Study ]
%       ]
%       \edge node[midway,right] {yes};
%       [.{Lazy ?} 
%         \edge node[midway,left] {no};
%         [.{?} ]
%         \edge node[midway,left] {yes};
%         [.Party ]
%       ]
%    ]
%    \edge node[auto=left] {Near};
%    [.{Party ?} 
%        \edge node[midway,left] {no};
%        [.{Lazy ?} 
%         \edge node[midway,left] {no};
%         [.Study ]
%         \edge node[midway,left] {yes};
%         [.TV ]
%        ]
%        \edge node[midway,right] {yes};
%        [.{Lazy ?} 
%         \edge node[midway,left] {no};
%         [.{?} ]
%         \edge node[midway,left] {yes};
%         [.Party ]
%        ]
%    ]
%    \edge node[auto=left] {None};
%    [.{Party ?} 
%        \edge node[midway,left] {no};
%        [.{Lazy ?} 
%         \edge node[midway,left] {no};
%         [.{?} ]
%         \edge node[midway,left] {yes};
%         [.Party ]
%        ]
%        \edge node[midway,right] {yes};
%        [.{Lazy ?} 
%         \edge node[midway,left] {no};
%         [.Party ]
%         \edge node[midway,left] {yes};
%         [.{?} ]
%        ]
%    ]
%]
%\end{tikzpicture}

\subsection{d)}
\subsubsection{i)}
\begin{tabular}{|c|c|c|}
\hline
Nr. & Entscheidungsbaum Entscheidung & Testdatenergebnis \\
\hline
\rowcolor{red} 1 & Study & TV \\
\hline
\rowcolor{green} 2 & Study & Study \\
\hline
\rowcolor{red} 3 & PUB & TV \\
\hline
\rowcolor{green} 4 & Party & Party \\
\hline
\rowcolor{red} 5 & PUB & Study \\
\hline
\end{tabular}

$40\%$ sind richtig und $60\%$ sind falsch.

\subsubsection{ii)}
\label{subsubsec:2d}

\includegraphics[scale=0.9]{2d2}

Nur beim 1. Testdatum kommt jetzt das gewünschte Ergebnis, somit sind $60\%$ richtig und $40\%$ falsch.

\subsubsection{iii)}

\includegraphics[scale=1]{2d3} \\
Das ergebniss ist wie in \nameref{subsubsec:2d}.

\section{Exercise 3 (Unsupervised learning – k-means clustering):}

\subsection{a)}
\label{subsec:kMeansClust}

\begin{table}[H]
\resizebox{\textwidth}{!}{
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|}
\hline
          &  $x^{(1)}$ & $x^{(2)}$ & $x^{(3)}$ & $x^{(4)}$ & $x^{(5)}$ & $x^{(6)}$ & $x^{(7)}$ & $x^{(8)}$ \\
\hline
$x^{(1)}$ &  0.0 &  5.0 &  8.48528137424 &  3.60555127546 &  7.07106781187 &  7.21110255093 &  8.0622577483 &  2.2360679775 \\
\hline
$x^{(2)}$ &  5.0 &  0.0 &  6.0827625303 &  4.24264068712 &  5.0 &  4.12310562562 &  3.16227766017 &  4.472135955 \\
\hline
$x^{(3)}$ &  8.48528137424 &  6.0827625303 &  0.0 &  5.0 &  1.41421356237 &  2.0 &  7.28010988928 &  6.40312423743 \\
\hline
$x^{(4)}$ &  3.60555127546 &  4.24264068712 &  5.0 &  0.0 &  3.60555127546 &  4.12310562562 &  7.21110255093 &  1.41421356237 \\
\hline
$x^{(5)}$ &  7.07106781187 &  5.0 &  1.41421356237 &  3.60555127546 &  0.0 &  1.41421356237 &  6.7082039325 &  5.0 \\
\hline
$x^{(6)}$ &  7.21110255093 &  4.12310562562 &  2.0 &  4.12310562562 &  1.41421356237 &  0.0 &  5.38516480713 &  5.38516480713 \\
\hline
$x^{(7)}$ &  8.0622577483 &  3.16227766017 &  7.28010988928 &  7.21110255093 &  6.7082039325 &  5.38516480713 &  0.0 &  7.61577310586 \\
\hline
$x^{(8)}$ &  2.2360679775 &  4.472135955 &  6.40312423743 &  1.41421356237 &  5.0 &  5.38516480713 &  7.61577310586 &  0.0 \\
\hline
\end{tabular}
}
\end{table}

\begin{enumerate}
\item Wähle $k$ Cluster Mittelwerte hier gegeben.
$$
x^{(1)} = m^{(0)} = 
\begin{pmatrix}
7 \\ 4
\end{pmatrix}, 
x^{(4)} = m^{(1)} = 
\begin{pmatrix}
3.5 \\ 9
\end{pmatrix}, 
x^{(7)} = m^{(2)} = 
\begin{pmatrix}
1.5 \\ 3,5
\end{pmatrix}, 
$$




\begin{tikzpicture}
\begin{axis}[
axis y line=center,
        axis x line=middle, 
        axis on top=true,
        xmin=0,
        xmax=11,
        ymin=0,
        ymax=11,
        clip=false,
        grid=both
]
\addplot [only marks] table {
2 10
2  5
8  4   
5  8
7  5
6  4
1  2
4  9
};
\addplot [only marks, mark=o] table {
2   10
5   8
1 2
};

\node [below] at (axis cs:  2,  10) {$m^{(0)}$};
\node [above] at (axis cs:  5,  8) {$m^{(1)}$};
\node [right] at (axis cs:  1, 2) {$m^{(2)}$};

\end{axis}
\end{tikzpicture}


\item Erstelle cluster

Ermittel welche Datenpunkte zu welchem Cluster die Datenpunkte gehören.

\begin{table}[tch]
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
          & $x^{(1)}$ & $x^{(2)}$ & $x^{(3)}$       & $x^{(4)}$           & $x^{(5)}$         & $x^{(6)}$         & $x^{(7)}$        & $x^{(8)}$ \\
\hline
$m^{(0)}$ & $ \textbf{0.0} $   & $ 5.0 $   & $ 8.48528137424 $ & $ \textbf{3.60555127546} $ & $ 7.07106781187 $ & $ 7.21110255093 $ & $ 8.0622577483 $ & $ 2.2360679775 $ \\
\hline
$m^{(1)}$ & $ \textbf{3.60555127546} $ & $ 4.24264068712 $ & $ \textbf{5.0} $ & $ \textbf{0.0} $ & $ 3.60555127546 $ & $ \textbf{4.12310562562} $ & $ 7.21110255093 $ & $ \textbf{1.41421356237} $ \\
\hline
$m^{(2)}$ & $ 8.0622577483 $ & $ \textbf{3.16227766017} $ & $ 7.28010988928 $ & $ 7.21110255093 $ & $ 6.7082039325 $ & $ 5.38516480713 $ & $ \textbf{0.0} $ & $ 7.61577310586 $ \\
\hline
\end{tabular}
}
\end{table}

\begin{tikzpicture}
\begin{axis}[
axis y line=center,
        axis x line=middle, 
        axis on top=true,
        xmin=0,
        xmax=11,
        ymin=0,
        ymax=11,
        clip=false,
        grid=both
]
\addplot [only marks, color=red] table {
2 10
};
\addplot [only marks, color=green] table {
8  4   
5  8
7  5
6  4
4  9
};
\addplot [only marks, color=blue] table {
2  5
1  2
};
\addplot [only marks, mark=o] table {
2   10
5   8
1 2
};

\node [below] at (axis cs:  2,  10) {$m^{(0)}$};
\node [above] at (axis cs:  5,  8) {$m^{(1)}$};
\node [right] at (axis cs:  1, 2) {$m^{(2)}$};

\end{axis}
\end{tikzpicture}

\item Neue Mittelwerte für $k$ Cluster berechnen.
Neu Mittelwerte sind:
$$
\tilde{m}^{(0)} = 
\begin{pmatrix}
2 \\ 10
\end{pmatrix}
$$

$$
\tilde{m}^{(1)} = 
\frac{1}{5} 
\left(
\begin{pmatrix}
8 \\ 4   
\end{pmatrix} 
+
\begin{pmatrix}
5 \\ 8
\end{pmatrix} 
+
\begin{pmatrix}
7 \\  5
\end{pmatrix} 
+
\begin{pmatrix}
6 \\  4
\end{pmatrix} 
+
\begin{pmatrix}
4 \\  9
\end{pmatrix} 
\right) = \begin{pmatrix}
6 \\ 6
\end{pmatrix}
$$

$$
\tilde{m}^{(2)} = 
\frac{1}{2} \left(
\begin{pmatrix}
2 \\ 5 
\end{pmatrix} +
\begin{pmatrix}
1 \\ 2
\end{pmatrix}
\right) = \begin{pmatrix}
\nicefrac{3}{2} \\ \nicefrac{7}{2} 
\end{pmatrix} = \begin{pmatrix}
1,5 \\ 3,5
\end{pmatrix}
$$


\item Mittelwert einzeichnen

\begin{tikzpicture}
\begin{axis}[
axis y line=center,
        axis x line=middle, 
        axis on top=true,
        xmin=0,
        xmax=11,
        ymin=0,
        ymax=11,
        clip=false,
        grid=both
]
\addplot [only marks] table {
2 10
2  5
8  4   
5  8
7  5
6  4
1  2
4  9
};
\addplot [only marks, mark=o] table {
2   10
6   6
1.5 3.5
};

\node [below] at (axis cs:  2,  10) {$m^{(0)}$};
\node [above] at (axis cs:  6,  6) {$m^{(1)}$};
\node [right] at (axis cs:  1.5, 3.5) {$m^{(2)}$};

\end{axis}
\end{tikzpicture}

\item Datenpunkte in Cluster einteilen.

\begin{table}[tch]
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 & $x^{(1)}$ & $x^{(2)}$ & $x^{(3)}$ & $x^{(4)}$ & $x^{(5)}$ & $x^{(6)}$ & $x^{(7)}$ & $x^{(8)}$ \\
\hline
$m^{(1)}$ & $ 0.0 $ & $ 5.0 $ & $ 8.48528137424 $ & $ 3.60555127546 $ & $ 7.07106781187 $ & $ 7.21110255093 $ & $ 8.0622577483 $ & $ 2.2360679775 $ \\
\hline
$m^{(2)}$ & $ 5.65685424949 $ & $ 4.12310562562 $ & $ 2.82842712475 $ & $ 2.2360679775 $ & $ 1.41421356237 $ & $ 2.0 $ & $ 6.40312423743 $ & $ 3.60555127546 $ \\
\hline
$m^{(3)}$ & $ 6.5192024052 $ & $ 1.58113883008 $ & $ 6.5192024052 $ & $ 5.7008771255 $ & $ 5.7008771255 $ & $ 4.52769256907 $ & $ 1.58113883008 $ & $ 6.0415229868 $ \\
\hline
\end{tabular}
}
\end{table}

\item Datenpunkte einzeichnen

\begin{tikzpicture}
\begin{axis}[
axis y line=center,
        axis x line=middle, 
        axis on top=true,
        xmin=0,
        xmax=11,
        ymin=0,
        ymax=11,
        clip=false,
        grid=both
]
\addplot [only marks, color=red] table {
2 10
4  9
};
\addplot [only marks, color=green] table {
8  4   
5  8
7  5
6  4
};
\addplot [only marks, color=blue] table {
2  5
1  2
};
\addplot [only marks, mark=o] table {
2   10
6   6
1.5 3.5
};

\node [below] at (axis cs:  2,  10) {$m^{(0)}$};
\node [above] at (axis cs:  6,  6) {$m^{(1)}$};
\node [right] at (axis cs:  1.5, 3.5) {$m^{(2)}$};

\end{axis}
\end{tikzpicture}

\item Solange fortführen bis Clusterzugehörigkeit der Punkte sich nicht mehr ändern.

\end{enumerate}

\subsection{b)}

\begin{figure}[H]
\caption{Erwiterte Ausgabe von exercise3b.py}
\label{fig:exercise3b}
\VerbatimInput[baselinestretch=1,fontsize=\footnotesize]{3b.txt}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{3b.png}
\end{figure}

Das Ergebnis aus \nameref{subsec:kMeansClust} geht in die richtige Richtung, 
wenn die Iterationsanzahl in \nameref{subsec:kMeansClust} höher wäre, wäre das Ergebnis besser.
Der minimale Wert von \textit{max\_iter} ist $3$, wie in \autoref{fig:exercise3b} zu entnehmen ist.

,jedoch wird eine Iterationen mehr benötigt, um festzustellen, dass sich die Mittelwerte nicht ändern.

\subsection{c)}

\subsubsection{ K-Means ++ Initialisierung: }
\begin{minipage}{\textwidth}

\includegraphics[scale=0.4]{spaeth_01_2clu.png}
\includegraphics[scale=0.4]{spaeth_01_3clu.png}
\includegraphics[scale=0.4]{spaeth_01_4clu.png}
\captionof{figure}{Verschiedene Cluster Anzahlen im Vergleich}
\end{minipage}
\newline


Um ein gutes Clustering zu erhalten muss die Anzahl der Cluster stimmen. Es ist deutlich zu sehen, dass 3 die ideale Anzahl ist. Im Fall von 2 Clustern werden Punkte zusammengefasst, die nicht direkt zusammengehören und bei 4 Clustern werden Punkte getrennt, welche offensichtlich zusammengehören. 
\subsubsection{Random Initialisierung: }

\begin{minipage}{\textwidth}
	\includegraphics[scale=0.5]{spaeth_01_3clu_ran.png}
\includegraphics[scale=0.5]{spaeth_01_3clu_ran42.png}
    \captionof{figure}{np.random.seed(1) und np.random.seed(42) im Vergleich}
\end{minipage} \newline

Die initialen Bedingung scheinen eine große Rolle zu spielen für das Ergebnis des Clusterings. Bei schlechten initialen Zentren erhält meine eine schlechtes Clustering. K-Means-Clustering liefert nur ein lokales Optimum. \\

Als Folgerung ergibt sich, dass man die richtige Anzahl von Clustern benötigt und vernünftige Startzentren, anderen Falls kann man schlechte Lösungen erhalten.

\subsection{d)}
Die Punkte lassen sich mit dem K-Means-Clustering nicht gut clustern, weil es sich um Punkte handelt, die entlang von Geraden verteilt sind und nicht konzentrisch um einen Punkt.\\

\begin{minipage}{\textwidth}
\centering
	\includegraphics[scale=0.5]{spaeth_05_2clu.png}
	\hfill
	\includegraphics[scale=0.5]{spaeth_05_3clu.png}
    \captionof{figure}{Spaeth05 mit 2 und 3 Clustern}
\end{minipage}


\subsection{e)}



\begin{minipage}[c]{\textwidth}
\centering
	\includegraphics[scale=0.5]{vogel2farben.png}
	%\hfill
	\includegraphics[scale=0.5]{vogel16farben.png}
    \captionof{figure}{KMeans-Clustering mit 2 und 16 Farben}
\end{minipage}


\end{document}